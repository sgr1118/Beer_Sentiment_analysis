{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkKifkzA1Ox1WGVjyf8LMK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 목표\n","\n","### simpleT5 클론 코딩을 통하여 구조를 이해한다.\n","- 각 코드를 이해하기 위하여 주석으로 설명하기로한다.\n","\n","### 사용하는 프레임워크 및 라이브러리\n","- Pytorch Lightning == 1.5.10\n","- numpy\n","- pandas\n","- sentencepiece\n","- torch>=1.7.0,!=1.8.0\n","- transformers==4.16.2"],"metadata":{"id":"sxjsvNrH6Yce"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYber-r4meqn"},"outputs":[],"source":["# 라이브러리 불러오기\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    T5ForConditionalGeneration,\n","    MT5ForConditionalGeneration,\n","    ByT5Tokenizer,\n","    PreTrainedTokenizer,\n","    T5TokenizerFast as T5Tokenizer,\n","    MT5TokenizerFast as MT5Tokenizer,\n",")\n","from transformers import AutoTokenizer\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks.progress import TQDMProgressBar\n","\n","torch.cuda.empty_cache()\n","pl.seed_everything(42)\n","\n","# 데이터셋 모듈\n","\n","class PytorchDataModule(Dataset):\n","    \"\"\" Pytorch Dataset class \"\"\"\n","\n","    def __init__(\n","        self,\n","        data : pd.DataFrame,\n","        tokenizer : PreTrainedTokenizer,\n","        source_max_token_len: int = 512, # 최대 토큰 길이\n","        target_max_token_len: int = 512, # 최대 토큰 길이\n","    ):\n","        \"\"\"\n","        initiates a PyTorch Dataset Module for input data\n","        Args:\n","            data (pd.DataFrame): input pandas dataframe. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n","            tokenizer (PreTrainedTokenizer): a PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n","            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n","            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n","        \"\"\"\n","\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.source_max_token_len = source_max_token_len\n","        self.target_max_token_len = target_max_token_len\n","\n","    def __len__(self):\n","        \"\"\" 데이터 길이 반환 \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, index: int):\n","        \"\"\" 이 코드는 T5/MT5 모델에 입력으로 사용될 수 있는 텐서 딕셔너리를 반환하는 함수 \"\"\"\n","        data_row = self.data.iloc[index]\n","        source_text = data_row[\"source_text\"]\n","\n","        source_text_encoding = self.tokenizer(\n","            source_text,\n","            max_legnth = self.source_max_token_len,\n","            padding = \"max_legnth\",\n","            truncation = True, # max_legnth 보다 긴 문장은 자른다.\n","            return_attention_mask = True,\n","            add_special_tokens = True,\n","            return_tensors = \"pt\",\n","        )\n","\n","        target_text_encoding  = self.tokenizer(\n","            data_row[\"target_text\"],\n","            max_legnth = self.target_max_token_len,\n","            padding = \"max_legnth\",\n","            truncation = True,\n","            return_attention_mask = True,\n","            add_special_tokens = True,\n","            return_tensors = \"pt\",\n","        )\n","\n","        labels = target_text_encoding[\"input_ids\"]\n","        labels[labels == 0] = -100 # 레이블 텐서에서 모든 패딩 토큰을 -100으로 대체하여 \n","        # 모델이 패딩 토큰을 예측하지 않도록 한다.\n","\n","        return dict(\n","            source_text_input_ids = source_text_encoding[\"input_ids\"].flatten(),\n","            source_text_attention_mask = source_text_encoding[\"attention_mask\"].flatten(),\n","            labels = labels.flatten(),\n","            labels_attention_mask = target_text_encoding[\"attention_mask\"].flatten()\n","        ) # dict에 각 요소를 1차원으로 평탄화하여 저장\n","          # model 입력은 1차원 텐서로 들어가야하기 때문에 flatten()을 사용해준다.\n","\n","class LightningDataModule(pl.LightningDataModule):\n","    \"\"\" PyTorch Lightning data class \"\"\"\n","\n","    def __init__(\n","        self,\n","        train_df: pd.DataFrame,\n","        test_df: pd.DataFrame,\n","        tokenizer: PreTrainedTokenizer,\n","        batch_size: int = 4,\n","        source_max_token_len: int = 512, # 최대 토큰 길이\n","        target_max_token_len: int = 512, # 최대 토큰 길이\n","        num_workers: int = 2,\n","    ):\n","\n","        \"\"\"\n","        initiates a PyTorch Lightning Data Module\n","        Args:\n","            train_df (pd.DataFrame): training dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n","            test_df (pd.DataFrame): validation dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n","            tokenizer (PreTrainedTokenizer): PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n","            batch_size (int, optional): batch size. Defaults to 4.\n","            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n","            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.train_df = train_df\n","        self.test_df = test_df\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.source_max_token_len = source_max_token_len\n","        self.target_max_token_len = target_max_token_len\n","        self.num_workers = num_workers\n","\n","    def setup(self, stage = None):\n","        self.train_dataset = PytorchDataModule(\n","            self.train_df,\n","            self.tokenizer,\n","            self.source_max_token_len,\n","            self.target_max_token_len,\n","        )\n","\n","        self.test_dataset = PytorchDataModule(\n","            self.test_df,\n","            self.tokenizer,\n","            self.source_max_token_len,\n","            self.target_max_token_len,\n","        )\n","\n","    def train_dataloader(self):\n","        \"\"\" training dataloader \"\"\"\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size = self.batch_size,\n","            shuffle = True,\n","            num_workers = self.num_workers,\n","        )\n","\n","    def test_dataloader(self):\n","        \"\"\" test dataloader \"\"\"\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","        )\n","\n","    def val_dataloader(self):\n","        \"\"\" validation dataloader \"\"\"\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","        )\n","\n","class LightningModel(pl.LightningModule):\n","    \"\"\" PyTorch Lightning Model class\"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer,\n","        model,\n","        outputdir: str = \"outputs\", # outputs dir 이름\n","        save_only_last_epoch: bool = False\n","    ):\n","        \"\"\"\n","        initiates a PyTorch Lightning Model\n","        Args:\n","            tokenizer : T5/MT5/ByT5 tokenizer\n","            model : T5/MT5/ByT5 model\n","            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n","            save_only_last_epoch (bool, optional): If True, save just the last epoch else models are saved for every epoch\n","        \"\"\"\n","        super().__init__()\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.outputdir = outputdir\n","        self.average_training_loss = None\n","        self.average_validation_loss = None\n","        self.save_only_last_epoch = save_only_last_epoch\n","\n","    def forward(self, input_ids, attention_mask,decoder_attention_mask, labels=None):\n","        \"\"\" forward step \"\"\"\n","        output = self.model(\n","            input_ids, # 토큰 ID 시퀀스\n","            attention_mask = attention_mask, # 각 토큰의 유효성 여부를 나타내는 마스킹 시퀀스\n","            labels = labels,\n","            decoder_attention_mask = decoder_attention_mask,\n","        )\n","        return output.loss, output.logits\n","        #logit은 모델이 예측한 값으로, 확률이 아닌 숫자값으로 출력됩니다. 이 값을 확률값으로 변환하여 예측값을 계산\n","        \n","    def training_step(self, batch, batch_size): # LightningDataModule에서 생성한 데이터 로더(dataloader)에서 반환된 batch\n","        \"\"\" training step \"\"\"\n","        input_ids = batch[\"source_text_input_ids\"]\n","        attention_mask = batch['source_text_attention_mask']\n","        labels = batch[\"labels\"]\n","        labels_attention_mask = batch[\"labels_attention_mask\"]\n","\n","        loss, outputs = self(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            decoder_attention_mask = labels_attention_mask,\n","            labels = labels,\n","        )\n","\n","        self.log(\n","            \"train_loss\", loss, prog_ar = True, logger = True, op_epoch = True, on_step = True\n","        )\n","        return loss # 손실 저장\n","\n","    def validation_step(self, batch, batch_size):\n","        \"\"\" validation step \"\"\"\n","        input_ids = batch[\"source_text_input_ids\"]\n","        attention_mask = batch[\"source_text_attention_mask\"]\n","        labels = batch[\"labels\"]\n","        labels_attention_mask = batch[\"labels_attention_mask\"]\n","\n","        loss, outputs = self(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_attention_mask=labels_attention_mask,\n","            labels=labels,\n","        )\n","\n","        self.log(\n","            \"val_loss\", loss, prog_bar = True, logger = True, on_epoch = True, on_step = True\n","        )\n","        return loss\n","\n","    def test_step(self, batch, batch_size):\n","        \"\"\" test step \"\"\"\n","        input_ids = batch[\"source_text_input_ids\"]\n","        attention_mask = batch[\"source_text_attention_mask\"]\n","        labels = batch[\"labels\"]\n","        labels_attention_mask = batch[\"labels_attention_mask\"]\n","\n","        loss, outputs = self(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_attention_mask=labels_attention_mask,\n","            labels=labels,\n","        )\n","\n","        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        \"\"\" configure optimizers \"\"\"\n","        return AdamW(self.parameters(), lr=0.0001)\n","        # AdamW : Adam optimizer의 변형으로, 가중치 감쇠(weight decay)를 적용하여 모델 파라미터의 크기를 줄이는 효과\n","\n","    def training_epoch_end(self, training_step_outputs):\n","        \"\"\" save tokenizer and model on epoch end \"\"\"\n","        self.average_training_loss = np.round(\n","            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n","            4,\n","        )\n","        path = f\"{self.outputdir}/simplet5-epoch-{self.current_epoch}-train-loss-{str(self.average_training_loss)}-val-loss-{str(self.average_validation_loss)}\"\n","\n","        if self.save_only_last_epoch:\n","            if self.current_epoch == self.trainer.max_epochs - 1:\n","                self.tokenizer.save_pretrained(path)\n","                self.model.save_pretrained(path)\n","        else:\n","            self.tokenizer.save_pretrained(path)\n","            self.model.save_pretrained(path)\n","\n","    def validation_epoch_end(self, validation_step_outputs):\n","        _loss = [x.cpu() for x in validation_step_outputs]\n","        self.average_validation_loss = np.round(\n","            torch.mean(torch.stack(_loss)).item(),\n","            4,\n","        )\n","\n","class SimpleT5:\n","    \"\"\" Custom SimpleT5 class \"\"\"\n","\n","    def __init__(self) -> None:\n","        \"\"\" initiates SimpleT5 class \"\"\"\n","        pass\n","\n","    def from_pretrained(self, model_type = \"t5\", model_name = \"t5-base\") -> None:\n","        \"\"\"\n","        loads T5/MT5 Model model for training/finetuning\n","        Args:\n","            model_type (str, optional): \"t5\" or \"mt5\" . Defaults to \"t5\".\n","            model_name (str, optional): exact model architecture name, \"t5-base\" or \"t5-large\". Defaults to \"t5-base\".\n","        \"\"\"\n","\n","        if model_type == \"t5\":\n","            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_name}\")\n","            self.model = T5ForConditionalGeneration.from_pretrained(\n","                f\"{model_name}\", return_dict=True\n","            )\n","        elif model_type == \"mt5\":\n","            self.tokenizer = MT5Tokenizer.from_pretrained(f\"{model_name}\")\n","            self.model = MT5ForConditionalGeneration.from_pretrained(\n","                f\"{model_name}\", return_dict=True\n","            )\n","        elif model_type == \"byt5\":\n","            self.tokenizer = ByT5Tokenizer.from_pretrained(f\"{model_name}\")\n","            self.model = T5ForConditionalGeneration.from_pretrained(\n","                f\"{model_name}\", return_dict=True\n","            )\n","\n","    def train(\n","        self,\n","        train_df: pd.DataFrame,\n","        eval_df: pd.DataFrame,\n","        source_max_token_len: int = 512,\n","        target_max_token_len: int = 512,\n","        batch_size: int = 8,\n","        max_epochs: int = 5,\n","        use_gpu: bool = True,\n","        outputdir: str = \"outputs\",\n","        early_stopping_patience_epochs: int = 0,  # 0 to disable early stopping feature\n","        precision=32, # 모델 훈련에 사용할 데이터 타입의 정밀도\n","        logger=\"default\",\n","        dataloader_num_workers: int = 2,\n","        save_only_last_epoch: bool = False,\n","    ):\n","        \"\"\"\n","        trains T5/MT5 model on custom dataset\n","        Args:\n","            train_df (pd.DataFrame): training datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n","            eval_df ([type], optional): validation datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n","            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n","            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n","            batch_size (int, optional): batch size. Defaults to 8.\n","            max_epochs (int, optional): max number of epochs. Defaults to 5.\n","            use_gpu (bool, optional): if True, model uses gpu for training. Defaults to True.\n","            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n","            early_stopping_patience_epochs (int, optional): monitors val_loss on epoch end and stops training, if val_loss does not improve after the specied number of epochs. set 0 to disable early stopping. Defaults to 0 (disabled)\n","            precision (int, optional): sets precision training - Double precision (64), full precision (32) or half precision (16). Defaults to 32.\n","            logger (pytorch_lightning.loggers) : any logger supported by PyTorch Lightning. Defaults to \"default\". If \"default\", pytorch lightning default logger is used.\n","            dataloader_num_workers (int, optional): number of workers in train/test/val dataloader\n","            save_only_last_epoch (bool, optional): If True, saves only the last epoch else models are saved at every epoch\n","        \"\"\"\n","\n","        self.data_module = LightningDataModule(\n","            train_df,\n","            eval_df,\n","            self.tokenizer,\n","            batch_size=batch_size,\n","            source_max_token_len=source_max_token_len,\n","            target_max_token_len=target_max_token_len,\n","            num_workers=dataloader_num_workers,\n","        )\n","        \n","        self.T5Model = LightningModel(\n","            tokenizer=self.tokenizer,\n","            model=self.model,\n","            outputdir=outputdir,\n","            save_only_last_epoch=save_only_last_epoch,\n","        )\n","\n","        # add callbacks\n","        callbacks = [TQDMProgressBar(refresh_rate = 5)]\n","\n","        if early_stopping_patience_epochs > 0:\n","            "]}]}