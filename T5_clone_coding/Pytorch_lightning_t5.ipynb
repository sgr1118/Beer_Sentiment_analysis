{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42WUZK47kWf",
        "outputId": "63b0dd58-71c1-4ae2-ac44-c28569eba7b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjQdifH2eWmW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIoserLd76jQ",
        "outputId": "8fceaf19-e327-446f-d2c0-19a01c632096"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kff-oic8ehCP",
        "outputId": "ad2082fa-581b-4ff7-b443-8ab100bb21ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed): # 랜덤 적용\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "ekiPPqGRe4hB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델링\n",
        "- 파이토치 라이트닝을 사용한다."
      ],
      "metadata": {
        "id": "NiyOXFdrfKdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5FineTuner, self).__init__() #  모델과 토크나이저를 불러옵니다.\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self): # 현재 모델 학습을 수행하는 프로세스의 랭크(rank)\n",
        "        return self.trainer.proc_rank <= 0 \n",
        "        # 첫 번째 프로세스에서만 로그를 출력하도록 하여 로깅을 중복되지 않도록 하는데 사용된다.\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None,\n",
        "        lm_labels = None): # 순전파\n",
        "\n",
        "        return self.model(input_ids, \n",
        "                          attention_mask = attention_mask, \n",
        "                          decoder_input_ids = decoder_input_ids,\n",
        "                          decoder_attention_mask = decoder_attention_mask, \n",
        "                          lm_labels = lm_labels)\n",
        "        \n",
        "    def _step(self, batch): # 모델에 입력을 주고 손실을 계산한 후 손실값을 반환\n",
        "        lm_labels = batch['target_ids'] # 입력 데이터에서 생성된 타겟 시퀀스를 저장\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100 # 모델이 패딩 토큰을 예측하지 못하도록 막는다.\n",
        "\n",
        "        outputs = self(input_ids = batch['source_ids'],\n",
        "                    attention_mask = batch['source_mask'],\n",
        "                    lm_labels = lm_labels,\n",
        "                    decoder_attention_mask=batch['target_mask'])\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss # 손실값 반환\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {'train_loss' : loss} # 학습 단계에서의 손실값을 계산합니다. \n",
        "        return {'loss' : loss, 'log' : tensorboard_logs} # tensorboard에 로그를 기록하고 손실값을 반환합니다.\n",
        "\n",
        "    def training_epoch_end(self, outputs): # epoch마다 계산된 손실값의 평균값을 반환한다.\n",
        "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'avg_train_loss' : avg_train_loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx): #  검증 단계에서의 손실값을 계산합니다.\n",
        "        loss = self._step(batch)\n",
        "        return {'val_loss' : loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs): # epoch마다 계산된 검증 손실값의 평균값을 반환한다.\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss' : avg_loss}\n",
        "        return {'avg_val_loss' : avg_loss, 'log' : tensorboard_logs, 'progress_bar' : tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # 옵티마이저와 스케줄러를 정의한다.\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params' : [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay' : self.hparams.weight_decay}, # 가중치 감쇠 적용\n",
        "\n",
        "             {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": 0.0} # 이 그룹에 속한 파라미터는 가중치 감쇠를 적용하지 않는다\n",
        "        ] # 각 파라미터 그업에 대해 서로 다른 가중치 감쇠를 적용한다.\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams.leraning_rate, eps = self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def get_tqdm_dict(self): #  tqdm에 표시할 딕셔너리를 반환합니다.\n",
        "        tqdm_dict = {'loss' : '{:.3f}'.format(self.trainer.avg_loss), 'lr' : self.lr_schedulers.get_last_lr()[-1]}\n",
        "        # 평균 손실 값을 소수점 세 자리까지 표시하고, 현재의 학습률을 표시\n",
        "\n",
        "        return tqdm_dict\n",
        "\n",
        "    def train_dataloader(self): # 학습 데이터 정의\n",
        "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size = self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
        "\n",
        "        t_total = (\n",
        "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "            // self.hparams.gradient_accumulation_steps * float(self.hparams.num_train_epoch)\n",
        "        )\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps = self.hparams.warmup_steps, num_training_steps = t_total\n",
        "        ) # get_linear_schedule_with_warmup : 옵티마이저와 학습률을 설정한다.\n",
        "          # 초기에 0에서 시작하여 지정된 에포크마다 선형적으로 증가하고 단계를 거친 후 다시 선형적으로 감소시키는 방식으로 학습률을 조정한다.\n",
        "          # num_warmup_steps : warmup 단계의 epoch \n",
        "          # num_training_steps : 전체 학습을 마칠 때까지 optimizer가 업데이트되는 횟수\n",
        "        self.lr_schedulers = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self): # 검증 데이터 정의\n",
        "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size = self.hparams.eval_batch_size, num_workers = 4)"
      ],
      "metadata": {
        "id": "P2n_ZMnRfO9P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# on_validation_end, on_test_end 오버라이드\n",
        "\n",
        "class LoggingCallback(pl.Callback): # 학습 중 결과를 로깅하고 파일에 저장한다.\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        logger.info('-- Validation results --')\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "\n",
        "            # 로그 결과\n",
        "            for key in sorted(metrics):\n",
        "                if key not in ['log', 'pregress_bar']:\n",
        "                    logger.info('{} = {}\\n'.format(key, str(metrics[key])))\n",
        "\n",
        "    def on_test_end(self, trainer, pl_module):\n",
        "        logger.info('-- Test results --')\n",
        "\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "\n",
        "            # 로그외 결과를 파일로 저장한다.\n",
        "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "            with open(output_test_results_file, \"w\") as writer:\n",
        "                for key in sorted(metrics):\n",
        "                    if key not in [\"log\", \"progress_bar\"]:\n",
        "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "metadata": {
        "id": "4774bxW2y-K_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 하이퍼 매개변수 및 기타 인수를 정의해 보겠습니다. 필요에 따라 특정 작업에 대해 이 사전을 재정의할 수 있습니다. 대부분의 경우 data_dir 및 output_dir만 변경하면 됩니다.\n",
        "\n",
        "### 여기서 배치 크기는 8이고 gradient_accumulation_steps는 16이므로 효과적인 배치 크기는 128입니다."
      ],
      "metadata": {
        "id": "4o5xBhhB08ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args_dict = dict(\n",
        "    data_dir = '', # 데이터 경로\n",
        "    output_dir = '', # 체크포인트 경로\n",
        "    model_name_or_path = 't5-base',\n",
        "    tokenizer_name_or_path = 't5-base',\n",
        "    max_seq_length = 512, # 입력 시퀀스의 최대 길이\n",
        "    learning_rate = 3e-4, # 학습률\n",
        "    weight_decay = 0.0, # L2 가중치 감쇠를 위한 파라미터\n",
        "    adam_epsilon = 1e-8, # Adam optimizer에서 epsilon 값\n",
        "    warmup_steps = 0, # learning rate scheduler에서 warmup steps\n",
        "    train_batch_size = 8, # 훈련 배치 사이즈\n",
        "    eval_train_size = 8, # 평가 배치 사이즈\n",
        "    num_train_epochs = 2, # 에포크 수\n",
        "    gradient_accumulation_steps = 16, # accumulation_steps의 수\n",
        "    n_gpu = 1, # 사용할 gpu 수\n",
        "    early_stop_callback = False, # early stopping 여부\n",
        "    fp_16 = False, # mixed precision training 사용 여부\n",
        "    opt_level = '01', # mixed precision training 시 사용할 최적화 레벨\n",
        "    max_grad_norm = 1.0, # gradient clipping을 위한 최대 그래디언트 norm 값\n",
        "    seed = 42 # random seed 값\n",
        ")"
      ],
      "metadata": {
        "id": "lHMKBn7I06NE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xvf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "nZpRrVLw6abF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_files = glob.glob('aclImdb/train/pos/*.txt')\n",
        "train_neg_files = glob.glob('aclImdb/train/neg/*.txt')"
      ],
      "metadata": {
        "id": "CuG52LmF6emd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pos_files), len(train_neg_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MBzvszw6fRu",
        "outputId": "78f32e49-097f-41d9-8386-bceb6b6cad63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 검증을 위하여 훈련 데이터 셋에서 2000개를 추출하고 긍정/부정 리뷰를 각 1000개씩 val 디렉토리에 저장한다."
      ],
      "metadata": {
        "id": "2QDFyKlS6o2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir aclImdb/val aclImdb/val/pos aclImdb/val/neg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFDP5UAZ6oVK",
        "outputId": "ddefdd64-ec7a-4750-fce9-898ec19b948b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘aclImdb/val’: File exists\n",
            "mkdir: cannot create directory ‘aclImdb/val/pos’: File exists\n",
            "mkdir: cannot create directory ‘aclImdb/val/neg’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(train_pos_files)\n",
        "random.shuffle(train_neg_files)\n",
        "\n",
        "val_pos_files = train_pos_files[:1000]\n",
        "val_neg_files = train_neg_files[:1000]"
      ],
      "metadata": {
        "id": "cHNwiOSr62Py"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "bb-EmaRx66iq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 이동\n",
        "\n",
        "for f in val_pos_files: \n",
        "  shutil.move(f,  'aclImdb/val/pos')\n",
        "for f in val_neg_files:\n",
        "  shutil.move(f,  'aclImdb/val/neg')"
      ],
      "metadata": {
        "id": "X0RXnWMA67WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 준비"
      ],
      "metadata": {
        "id": "-q3VFiqW7Ota"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks4NBFIf7SUF",
        "outputId": "5dc4aba0-c360-43ad-fa58-47138378447a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정 토큰화\n",
        "\n",
        "ids_neg = tokenizer.encode('negative </s>')\n",
        "ids_pos = tokenizer.encode('positive </s>')\n",
        "print(len(ids_neg), len(ids_pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH0ysuvZ8184",
        "outputId": "3e7d1005-75d9-465b-a2d1-c0d80102729b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 리뷰가 긍정적이라면 긍정적, 부정적이라면 부정적으로 인코딩된다.\n",
        "\n",
        "### html 태그를 제거하여 리뷰 텍스트를 정리한다. 또한 T5 모델에서 요구하는 입력 및 대상 끝에 eos 토큰 </s>를 추가한다."
      ],
      "metadata": {
        "id": "UcYKu-M89UcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImdbDataset(Dataset): # 리뷰 데이터셋을 처리하는 데이터셋 클래스\n",
        "    def __init__(self, tokenizer, data_dir, type_path, max_len = 512): # () 안에 인스턴스 변수 초기화\n",
        "        self.pos_file_path = os.path.join(data_dir, type_path, 'pos')\n",
        "        self.neg_file_path = os.path.join(data_dir, type_path, 'neg')\n",
        "\n",
        "        self.pos_files = glob.glob(\"%s/*.txt\" % self.pos_file_path)\n",
        "        self.neg_files = glob.glob(\"%s/*.txt\" % self.neg_file_path)\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self): # inputs 길이 반환\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index): # source_ids, source_mask, target_ids, target_mask를 딕셔너리 형태로 반환\n",
        "    # source_ids와 target_ids는 squeeze를 사용하여 1차원으로 변경\n",
        "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
        "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index]['attention_mask'].squeeze()\n",
        "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
        "\n",
        "        return {'source_ids' : source_ids, 'source_mask' : src_mask, 'target_ids' : target_ids, 'target_mask' : target_mask}\n",
        "\n",
        "    def _build(self): # 해당 폴더에서 파일을 가져와 self._buil_examples_from_files 메서드 호출\n",
        "        self._build_examples_from_files(self.pos_files, 'positive')\n",
        "        self._build_examples_from_files(self.neg_files, 'negative')\n",
        "\n",
        "    def _build_examples_from_files(self, files, sentiment): # 파일을 받아와 전처리 적용\n",
        "        REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "        REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "        for path in files:\n",
        "            with open(path, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            line = text.strip()\n",
        "            line = REPLACE_NO_SPACE.sub('', line)\n",
        "            line = REPLACE_WITH_SPACE.sub('', line)\n",
        "            line = line + ' </s>'\n",
        "\n",
        "            target = sentiment + ' </s>'\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [line], max_length = self.max_len, pad_to_max_length = True, return_tensors = 'pt'\n",
        "            )\n",
        "\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length = 2, pad_to_max_length = True, return_tensors = 'pt'\n",
        "            ) # max_length가 2인 이유는 감성 레이블을 대표하는 토큰이기 때문이다.\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.inputs.append(tokenized_targets)"
      ],
      "metadata": {
        "id": "k5E-aYKY9T68"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImdbDataset(tokenizer, 'aclImdb', 'val',  max_len=512)\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OiKaV9y9MbY",
        "outputId": "b4578fe3-b80b-46ce-9f14-a6c3910e33e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련"
      ],
      "metadata": {
        "id": "Wsv8IYcpFJHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p t5_imdb_sentiment"
      ],
      "metadata": {
        "id": "dsAsZpd8DigO"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}